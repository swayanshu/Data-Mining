{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import codecs\n",
    "from glob import glob\n",
    "from datetime import datetime\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 16, 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA = \"../input/sleep-state\"\n",
    "PP_DATA = r\"D:\\2nd_semester\\data_mining\\Project\\Data_demo\"\n",
    "WINDOW_SIZE = 100\n",
    "bs = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install git+https://www.github.com/keras-team/keras-contrib.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv1D, Dense, Dropout, MaxPool1D, Activation\n",
    "from tensorflow.keras.layers import Reshape, LSTM, TimeDistributed, Bidirectional, BatchNormalization, Flatten, RepeatVector\n",
    "#from tensorflow.keras.layers import CuDNNGRU, CuDNNLSTM\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report, roc_auc_score, confusion_matrix, roc_auc_score, roc_curve\n",
    "\n",
    "import mne\n",
    "from mne.io import read_raw_edf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\2nd_semester\\\\data_mining\\\\Project\\\\Data_demo\\\\SC4001E0.txt.npz'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnames = sorted(glob(os.path.join(PP_DATA, \"*.npz\")))\n",
    "len(fnames)\n",
    "fnames[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 D:\\2nd_semester\\data_mining\\Project\\Data_demo\\SC4001E0.txt.npz\n"
     ]
    }
   ],
   "source": [
    "#Data generator\n",
    "total_fs = [f for f in fnames if f.split(\"/\")[-1][:5]]\n",
    "print(len(total_fs), total_fs[0])\n",
    "total_data = {k: np.load(k) for k in total_fs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(841, 3000, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = total_data[r\"D:\\2nd_semester\\data_mining\\Project\\Data_demo\\SC4001E0.txt.npz\"]\n",
    "samples['x'].shape  # (n_epochs, sample_len, n_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(841, 3000, 1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = total_data[\"C:/Users/pratik/Desktop/DM Project/New folder/np\\SC4001E0.npz\"]\n",
    "samples['x'].shape  # (n_epochs, sample_len, n_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((0, 3000, 1))\n",
    "y = []\n",
    "for fn in total_fs:\n",
    "    samples = np.load(fn)\n",
    "    X_data = samples[\"x\"]\n",
    "    X = np.concatenate((X, X_data), axis=0)\n",
    "    y.extend(samples[\"y\"])\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = samples['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(841, 3000)\n",
      "(3,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.1,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    test_size=0.1,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((680, 3000, 1), (85, 3000, 1), (76, 3000, 1))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale_array(X):\n",
    "    X = X / 20\n",
    "    X = np.clip(X, -5, 5)\n",
    "    return X\n",
    "\n",
    "def aug_X(X):\n",
    "    scale = 1 + np.random.uniform(-0.1, 0.1)\n",
    "    offset = np.random.uniform(-0.1, 0.1)\n",
    "    noise = np.random.normal(scale=0.05, size=X.shape)\n",
    "    X = scale * X + offset + noise\n",
    "    return X\n",
    "\n",
    "def gen(dict_files, scale=True, aug=False):\n",
    "    while True:\n",
    "        record_name = random.choice(list(dict_files.keys()))\n",
    "        batch_data = dict_files[record_name]\n",
    "        all_rows = batch_data['x']\n",
    "\n",
    "        for i in range(10):\n",
    "            start_index = random.choice(range(all_rows.shape[0]-WINDOW_SIZE))\n",
    "\n",
    "            X = all_rows[start_index:start_index+WINDOW_SIZE, ...]\n",
    "            Y = batch_data['y'][start_index:start_index+WINDOW_SIZE]\n",
    "\n",
    "            X = np.expand_dims(X, 0)\n",
    "            Y = np.expand_dims(Y, -1)\n",
    "            Y = np.expand_dims(Y, 0)\n",
    "\n",
    "            if aug:\n",
    "                X = aug_X(X)\n",
    "                \n",
    "            if scale:\n",
    "                X = rescale_array(X)\n",
    "\n",
    "            yield np.squeeze(X, axis=0), np.squeeze(Y, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(X_samples, y_samples, bs=32, label=True):\n",
    "    i = 0\n",
    "#     X_samples = X_samples[:]\n",
    "#     y_samples = y_samples[:]\n",
    "    while True:\n",
    "        X_temp = X_samples[i:i + bs]\n",
    "        y_temp = y_samples[i:i + bs]\n",
    "        \n",
    "        X_temp = np.array([rescale_array(sample) for sample in X_temp])\n",
    "        y_temp = np.array(y_temp)\n",
    "        if label:\n",
    "            yield X_temp, y_temp\n",
    "        else:\n",
    "            yield X_temp\n",
    "\n",
    "        i += bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inLayer (InputLayer)            [(None, 3000, 1)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "fConv1 (Conv1D)                 (None, 500, 64)      3264        inLayer[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "cConv1 (Conv1D)                 (None, 60, 32)       12832       inLayer[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "fMaxP1 (MaxPooling1D)           (None, 62, 64)       0           fConv1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "cMaxP1 (MaxPooling1D)           (None, 15, 32)       0           cConv1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "fDrop1 (Dropout)                (None, 62, 64)       0           fMaxP1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "cDrop1 (Dropout)                (None, 15, 32)       0           cMaxP1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "fConv2 (Conv1D)                 (None, 62, 128)      65664       fDrop1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "cConv2 (Conv1D)                 (None, 15, 128)      24704       cDrop1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "fConv3 (Conv1D)                 (None, 62, 128)      131200      fConv2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "cConv3 (Conv1D)                 (None, 15, 128)      98432       cConv2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "fConv4 (Conv1D)                 (None, 62, 128)      131200      fConv3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "cConv4 (Conv1D)                 (None, 15, 128)      98432       cConv3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "fMaxP2 (MaxPooling1D)           (None, 15, 128)      0           fConv4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "cMaxP2 (MaxPooling1D)           (None, 7, 128)       0           cConv4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "fFlat1 (Flatten)                (None, 1920)         0           fMaxP2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "cFlat1 (Flatten)                (None, 896)          0           cMaxP2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "merge_1 (Concatenate)           (None, 2816)         0           fFlat1[0][0]                     \n",
      "                                                                 cFlat1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mDrop1 (Dropout)                (None, 2816)         0           merge_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape1 (Reshape)              (None, 1, 2816)      0           mDrop1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 256)          3015680     reshape1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 1, 256)       0           bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 256)          394240      reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "merge_out_sub (Dropout)         (None, 256)          0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "outLayer (Dense)                (None, 5)            1285        merge_out_sub[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 3,976,933\n",
      "Trainable params: 3,976,933\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#model\n",
    "Fs = 100\n",
    "\n",
    "def model_baseline_2017(n_classes=5, use_sub_layer=False, summary=True):\n",
    "    # two conv-nets in parallel for feature learning, \n",
    "    # one with fine resolution another with coarse resolution    \n",
    "    # network to learn fine features\n",
    "    inputLayer = Input(shape=(3000, 1), name='inLayer')\n",
    "    convFine = Conv1D(filters=64, kernel_size=int(Fs/2), strides=int(Fs/16), padding='same', activation='relu', name='fConv1')(inputLayer)\n",
    "    convFine = MaxPool1D(pool_size=8, strides=8, name='fMaxP1')(convFine)\n",
    "    convFine = Dropout(rate=0.5, name='fDrop1')(convFine)\n",
    "    convFine = Conv1D(filters=128, kernel_size=8, padding='same', activation='relu', name='fConv2')(convFine)\n",
    "    convFine = Conv1D(filters=128, kernel_size=8, padding='same', activation='relu', name='fConv3')(convFine)\n",
    "    convFine = Conv1D(filters=128, kernel_size=8, padding='same', activation='relu', name='fConv4')(convFine)\n",
    "    convFine = MaxPool1D(pool_size=4, strides=4, name='fMaxP2')(convFine)\n",
    "    fineShape = convFine.get_shape()\n",
    "    convFine = Flatten(name='fFlat1')(convFine)\n",
    "    \n",
    "    # network to learn coarse features\n",
    "    convCoarse = Conv1D(filters=32, kernel_size=Fs*4, strides=int(Fs/2), padding='same', activation='relu', name='cConv1')(inputLayer)\n",
    "    convCoarse = MaxPool1D(pool_size=4, strides=4, name='cMaxP1')(convCoarse)\n",
    "    convCoarse = Dropout(rate=0.5, name='cDrop1')(convCoarse)\n",
    "    convCoarse = Conv1D(filters=128, kernel_size=6, padding='same', activation='relu', name='cConv2')(convCoarse)\n",
    "    convCoarse = Conv1D(filters=128, kernel_size=6, padding='same', activation='relu', name='cConv3')(convCoarse)\n",
    "    convCoarse = Conv1D(filters=128, kernel_size=6, padding='same', activation='relu', name='cConv4')(convCoarse)\n",
    "    convCoarse = MaxPool1D(pool_size=2, strides=2, name='cMaxP2')(convCoarse)\n",
    "    coarseShape = convCoarse.get_shape()\n",
    "    convCoarse = Flatten(name='cFlat1')(convCoarse)\n",
    "    \n",
    "    # concatenate coarse and fine cnns\n",
    "    mergeLayer = concatenate([convFine, convCoarse], name='merge_1')\n",
    "    outLayer = Dropout(rate=0.5, name='mDrop1')(mergeLayer)\n",
    "    if use_sub_layer:\n",
    "        sub_layer = Dense(1024, activation=\"relu\", name='sub_layer')(outLayer)\n",
    "    # model = Model(inputLayer, mergeLayer)\n",
    "    \n",
    "    # LSTM\n",
    "    outLayer = Reshape((1, int(fineShape[1]*fineShape[2] + coarseShape[1]*coarseShape[2])), name='reshape1')(outLayer)\n",
    "    outLayer = Bidirectional(LSTM(128, activation='relu', dropout=0.5, name='bLstm1'))(outLayer)\n",
    "    outLayer = Reshape((1, int(outLayer.get_shape()[1])))(outLayer)\n",
    "    outLayer = Bidirectional(LSTM(128, activation='relu', dropout=0.5, name='bLstm2'))(outLayer)\n",
    "    \n",
    "    # merge out_layer and sub_layer\n",
    "    if use_sub_layer:\n",
    "        outLayer = concatenate([outLayer, sub_layer], name='merge_2')\n",
    "        outLayer = Dropout(rate=0.5, name='mDrop2')(outLayer)\n",
    "        outLayer = Dense(256, activation=\"relu\", name='sub_layer_2')(outLayer)\n",
    "    outLayer = Dropout(rate=0.5, name='merge_out_sub')(outLayer)\n",
    "    \n",
    "    # Classify\n",
    "    outLayer = Dense(n_classes, activation='softmax', name='outLayer')(outLayer)\n",
    "    model = Model(inputLayer, outLayer)\n",
    "    optimizer = keras.optimizers.Adam(lr=1e-4)\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['acc'])\n",
    "    \n",
    "    if summary:\n",
    "        model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "model_2017 = model_baseline_2017(use_sub_layer=False, summary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inLayer (InputLayer)            [(None, 3000, 1)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "fConv1 (Conv1D)                 (None, 500, 64)      3264        inLayer[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "cConv1 (Conv1D)                 (None, 60, 32)       12832       inLayer[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "fMaxP1 (MaxPooling1D)           (None, 62, 64)       0           fConv1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "cMaxP1 (MaxPooling1D)           (None, 15, 32)       0           cConv1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "fDrop1 (Dropout)                (None, 62, 64)       0           fMaxP1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "cDrop1 (Dropout)                (None, 15, 32)       0           cMaxP1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "fConv2 (Conv1D)                 (None, 62, 128)      65664       fDrop1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "cConv2 (Conv1D)                 (None, 15, 128)      24704       cDrop1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "fConv3 (Conv1D)                 (None, 62, 128)      131200      fConv2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "cConv3 (Conv1D)                 (None, 15, 128)      98432       cConv2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "fConv4 (Conv1D)                 (None, 62, 128)      131200      fConv3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "cConv4 (Conv1D)                 (None, 15, 128)      98432       cConv3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "fMaxP2 (MaxPooling1D)           (None, 15, 128)      0           fConv4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "cMaxP2 (MaxPooling1D)           (None, 7, 128)       0           cConv4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "fFlat1 (Flatten)                (None, 1920)         0           fMaxP2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "cFlat1 (Flatten)                (None, 896)          0           cMaxP2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "merge_1 (Concatenate)           (None, 2816)         0           fFlat1[0][0]                     \n",
      "                                                                 cFlat1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mDrop1 (Dropout)                (None, 2816)         0           merge_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape1 (Reshape)              (None, 1, 2816)      0           mDrop1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 1, 64)        737536      reshape1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 64)           33024       lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "outLayer (Dense)                (None, 5)            325         lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 1,336,613\n",
      "Trainable params: 1,336,613\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "fs = 100\n",
    "\n",
    "def model_baseline_2019(n_classes=5, use_sub_layer=False, use_rnn=True):\n",
    "    \"\"\"Recurrent_Deep_Neural_Networks_for_Real-Time_Sleep\n",
    "    \"\"\"\n",
    "    inputLayer = Input(shape=(3000, 1), name='inLayer')\n",
    "    convFine = Conv1D(filters=64, kernel_size=int(Fs/2), strides=int(Fs/16), padding='same', activation='relu', name='fConv1')(inputLayer)\n",
    "    convFine = MaxPool1D(pool_size=8, strides=8, name='fMaxP1')(convFine)\n",
    "    convFine = Dropout(rate=0.5, name='fDrop1')(convFine)\n",
    "    convFine = Conv1D(filters=128, kernel_size=8, padding='same', activation='relu', name='fConv2')(convFine)\n",
    "    convFine = Conv1D(filters=128, kernel_size=8, padding='same', activation='relu', name='fConv3')(convFine)\n",
    "    convFine = Conv1D(filters=128, kernel_size=8, padding='same', activation='relu', name='fConv4')(convFine)\n",
    "    convFine = MaxPool1D(pool_size=4, strides=4, name='fMaxP2')(convFine)\n",
    "    fineShape = convFine.get_shape()\n",
    "    convFine = Flatten(name='fFlat1')(convFine)\n",
    "    \n",
    "    # network to learn coarse features\n",
    "    convCoarse = Conv1D(filters=32, kernel_size=Fs*4, strides=int(Fs/2), padding='same', activation='relu', name='cConv1')(inputLayer)\n",
    "    convCoarse = MaxPool1D(pool_size=4, strides=4, name='cMaxP1')(convCoarse)\n",
    "    convCoarse = Dropout(rate=0.5, name='cDrop1')(convCoarse)\n",
    "    convCoarse = Conv1D(filters=128, kernel_size=6, padding='same', activation='relu', name='cConv2')(convCoarse)\n",
    "    convCoarse = Conv1D(filters=128, kernel_size=6, padding='same', activation='relu', name='cConv3')(convCoarse)\n",
    "    convCoarse = Conv1D(filters=128, kernel_size=6, padding='same', activation='relu', name='cConv4')(convCoarse)\n",
    "    convCoarse = MaxPool1D(pool_size=2, strides=2, name='cMaxP2')(convCoarse)\n",
    "    coarseShape = convCoarse.get_shape()\n",
    "    convCoarse = Flatten(name='cFlat1')(convCoarse)\n",
    "    \n",
    "    # concatenate coarse and fine cnns\n",
    "    mergeLayer = concatenate([convFine, convCoarse], name='merge_1')\n",
    "    outLayer = Dropout(rate=0.5, name='mDrop1')(mergeLayer)\n",
    "    \n",
    "    outLayer = Reshape((1, outLayer.get_shape()[1]), name='reshape1')(outLayer)\n",
    "    outLayer = LSTM(64, return_sequences=True)(outLayer)\n",
    "    outLayer = LSTM(64, return_sequences=False)(outLayer)\n",
    "\n",
    "    # Classify\n",
    "    outLayer = Dense(n_classes, activation='softmax', name='outLayer')(outLayer)\n",
    "    model = Model(inputLayer, outLayer)\n",
    "    optimizer = keras.optimizers.Adam(lr=1e-4)\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "model_2019 = model_baseline_2019(use_sub_layer=False, use_rnn=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_metrics(cm,classes):\n",
    "\n",
    "    print (\"Confusion matrix:\")\n",
    "    print (cm)\n",
    "\n",
    "    cm = cm.astype(np.float32)\n",
    "    FP = cm.sum(axis=0) - np.diag(cm)\n",
    "    FN = cm.sum(axis=1) - np.diag(cm)\n",
    "    TP = np.diag(cm)\n",
    "    TN = cm.sum() - (FP + FN + TP)\n",
    "    # https://stackoverflow.com/questions/31324218/scikit-learn-how-to-obtain-true-positive-true-negative-false-positive-and-fal\n",
    "    # Sensitivity, hit rate, recall, or true positive rate\n",
    "    TPR = TP / (TP + FN)\n",
    "    # Specificity or true negative rate\n",
    "    TNR = TN / (TN + FP)\n",
    "    # Precision or positive predictive value\n",
    "    PPV = TP / (TP + FP)\n",
    "    # Negative predictive value\n",
    "    NPV = TN / (TN + FN)\n",
    "    # Fall out or false positive rate\n",
    "    FPR = FP / (FP + TN)\n",
    "    # False negative rate\n",
    "    FNR = FN / (TP + FN)\n",
    "    # False discovery rate\n",
    "    FDR = FP / (TP + FP)\n",
    "\n",
    "    # Overall accuracy\n",
    "    ACC = (TP + TN) / (TP + FP + FN + TN)\n",
    "    # ACC_micro = (sum(TP) + sum(TN)) / (sum(TP) + sum(FP) + sum(FN) + sum(TN))\n",
    "    ACC_macro = np.mean(ACC) # to get a sense of effectiveness of our method on the small classes we computed this average (macro-average)\n",
    "\n",
    "    F1 = (2 * PPV * TPR) / (PPV + TPR)\n",
    "    F1_macro = np.mean(F1)\n",
    "\n",
    "    print (\"Sample: {}\".format(int(np.sum(cm))))\n",
    "    n_classes = len(classes)\n",
    "    for index_ in range(n_classes):\n",
    "        print (\"{}: {}\".format(classes[index_], int(TP[index_] + FN[index_])))\n",
    "\n",
    "\n",
    "    return ACC_macro,ACC, F1_macro, F1, TPR, TNR, PPV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train\n",
    "from scipy.signal import butter, lfilter\n",
    "\n",
    "def butter_bandpass(lowcut, highpass, fs, order=4):\n",
    "    nyq = 0.5 * fs\n",
    "    #       low = lowcut / nyq\n",
    "    high = highpass / nyq\n",
    "    b, a = butter(order, high, btype='highpass')\n",
    "    return b, a\n",
    "   \n",
    "def butter_bandpass_filter(data, highpass, fs, order=4):\n",
    "    b, a = butter_bandpass(0, highpass, fs, order=order)\n",
    "    y = lfilter(b, a, data)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([102.44688416]), array([-112.66812897]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(sample), min(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = butter_bandpass_filter(sample, highpass=40.0, fs=100, order=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.49423895]), array([-0.54354974]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(res), min(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(\"model_cps\", monitor='val_loss', verbose=1, save_best_only=True, mode='max')\n",
    "# early = EarlyStopping(monitor=\"val_loss\", mode=\"max\", patience=20, verbose=1)\n",
    "redonplat = ReduceLROnPlateau(monitor=\"val_loss\", mode=\"max\", patience=5, verbose=2)\n",
    "csv_logger = CSVLogger('log_training.csv', append=True, separator=',')\n",
    "callbacks_list = [\n",
    "    checkpoint,\n",
    "#     early,\n",
    "    redonplat,\n",
    "    csv_logger,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_ = to_categorical(y_train)\n",
    "y_val_ = to_categorical(y_val)\n",
    "y_test_ = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seq2seq'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-206-c3facfb0245c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mseq2seq\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'seq2seq'"
     ]
    }
   ],
   "source": [
    "import seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 680 samples, validate on 76 samples\n",
      "Epoch 1/30\n",
      "640/680 [===========================>..] - ETA: 0s - loss: 2.0639 - acc: 0.1937\n",
      "Epoch 00001: val_loss improved from -inf to 1.54399, saving model to model_cps\n",
      "WARNING:tensorflow:From D:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Attempted to save a function b'__inference_forward_bLstm1_layer_call_fn_24446' which references a symbolic Tensor Tensor(\"dropout/mul_1:0\", shape=(None, 2816), dtype=float32) that is not a simple constant. This is not supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-67bb763c8b4e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m hist = model_2017.fit(\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m )\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    395\u001b[0m                       total_epochs=1)\n\u001b[0;32m    396\u001b[0m                   cbks.make_logs(model, epoch_logs, eval_result, ModeKeys.TEST,\n\u001b[1;32m--> 397\u001b[1;33m                                  prefix='val_')\n\u001b[0m\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m                 \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[1;34m(self, epoch, mode)\u001b[0m\n\u001b[0;32m    769\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    770\u001b[0m         \u001b[1;31m# Epochs only apply to `fit`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 771\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    772\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    773\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    300\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m       \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    990\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    991\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 992\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    993\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    994\u001b[0m       \u001b[1;31m# For multi-worker training, back up the weights and current training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_save_model\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   1027\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1028\u001b[0m               \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1029\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1030\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\network.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, filepath, overwrite, include_optimizer, save_format, signatures, options)\u001b[0m\n\u001b[0;32m   1006\u001b[0m     \"\"\"\n\u001b[0;32m   1007\u001b[0m     save.save_model(self, filepath, overwrite, include_optimizer, save_format,\n\u001b[1;32m-> 1008\u001b[1;33m                     signatures, options)\n\u001b[0m\u001b[0;32m   1009\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1010\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0msave_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\save.py\u001b[0m in \u001b[0;36msave_model\u001b[1;34m(model, filepath, overwrite, include_optimizer, save_format, signatures, options)\u001b[0m\n\u001b[0;32m    113\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m     saved_model_save.save(model, filepath, overwrite, include_optimizer,\n\u001b[1;32m--> 115\u001b[1;33m                           signatures, options)\n\u001b[0m\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\saved_model\\save.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(model, filepath, overwrite, include_optimizer, signatures, options)\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[1;31m# we use the default replica context here.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mdistribution_strategy_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_default_replica_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m       \u001b[0msave_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msignatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0minclude_optimizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\saved_model\\save.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(obj, export_dir, signatures, options)\u001b[0m\n\u001b[0;32m    907\u001b[0m   \u001b[0mobject_saver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrackableSaver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheckpoint_graph_view\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    908\u001b[0m   asset_info, exported_graph = _fill_meta_graph_def(\n\u001b[1;32m--> 909\u001b[1;33m       meta_graph_def, saveable_view, signatures, options.namespace_whitelist)\n\u001b[0m\u001b[0;32m    910\u001b[0m   saved_model.saved_model_schema_version = (\n\u001b[0;32m    911\u001b[0m       constants.SAVED_MODEL_SCHEMA_VERSION)\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\saved_model\\save.py\u001b[0m in \u001b[0;36m_fill_meta_graph_def\u001b[1;34m(meta_graph_def, saveable_view, signature_functions, namespace_whitelist)\u001b[0m\n\u001b[0;32m    551\u001b[0m   \u001b[0mresource_initializer_ops\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m   \u001b[1;32mwith\u001b[0m \u001b[0mexported_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 553\u001b[1;33m     \u001b[0mobject_map\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresource_map\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0masset_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msaveable_view\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_resources\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    554\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mresource_initializer_function\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mresource_initializer_functions\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    555\u001b[0m       \u001b[0masset_dependencies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\saved_model\\save.py\u001b[0m in \u001b[0;36mmap_resources\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    283\u001b[0m                 (\"Attempted to save a function {} which references a symbolic \"\n\u001b[0;32m    284\u001b[0m                  \u001b[1;34m\"Tensor {} that is not a simple constant. This is not \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 285\u001b[1;33m                  \"supported.\").format(concrete_function.name, capture))\n\u001b[0m\u001b[0;32m    286\u001b[0m           \u001b[0mcopied_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcapture_constant_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m           \u001b[0mnode_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Attempted to save a function b'__inference_forward_bLstm1_layer_call_fn_24446' which references a symbolic Tensor Tensor(\"dropout/mul_1:0\", shape=(None, 2816), dtype=float32) that is not a simple constant. This is not supported."
     ]
    }
   ],
   "source": [
    "hist = model_2017.fit(\n",
    "    X_train, y_train_, batch_size=64, epochs=30, validation_data=(X_val, y_val_), callbacks=callbacks_list\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> f1 score: 0.24005352363960747\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        16\n",
      "           1       0.00      0.00      0.00         4\n",
      "           2       0.54      0.83      0.66        30\n",
      "           3       0.42      0.76      0.54        21\n",
      "           4       0.00      0.00      0.00        14\n",
      "\n",
      "    accuracy                           0.48        85\n",
      "   macro avg       0.19      0.32      0.24        85\n",
      "weighted avg       0.30      0.48      0.37        85\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model_2017.predict(X_test, batch_size=bs)\n",
    "y_pred = np.array([np.argmax(s) for s in y_pred])\n",
    "f1 = f1_score(y_test, y_pred, average=\"macro\")\n",
    "print(\">>> f1 score: {}\".format(f1))\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2017.save_weights(\"raw_model_2017.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4823529411764706"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2943618f088>]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hist.history[\"loss\"])\n",
    "plt.plot(hist.history[\"val_loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x294361f7b88>]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hist.history[\"acc\"])\n",
    "plt.plot(hist.history[\"val_acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_X_train = np.array([butter_bandpass_filter(sample, highpass=40.0, fs=100, order=4) for sample in X_train])\n",
    "pp_X_val = np.array([butter_bandpass_filter(sample, highpass=40.0, fs=100, order=4) for sample in X_val])\n",
    "pp_X_test = np.array([butter_bandpass_filter(sample, highpass=40.0, fs=100, order=4) for sample in X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_X_test = pp_X_test.reshape(76,3000,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_X_train = pp_X_train.reshape(765,3000,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_9\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inLayer (InputLayer)            [(None, 3000, 1)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "fConv1 (Conv1D)                 (None, 500, 64)      3264        inLayer[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "cConv1 (Conv1D)                 (None, 60, 32)       12832       inLayer[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "fMaxP1 (MaxPooling1D)           (None, 62, 64)       0           fConv1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "cMaxP1 (MaxPooling1D)           (None, 15, 32)       0           cConv1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "fDrop1 (Dropout)                (None, 62, 64)       0           fMaxP1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "cDrop1 (Dropout)                (None, 15, 32)       0           cMaxP1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "fConv2 (Conv1D)                 (None, 62, 128)      65664       fDrop1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "cConv2 (Conv1D)                 (None, 15, 128)      24704       cDrop1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "fConv3 (Conv1D)                 (None, 62, 128)      131200      fConv2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "cConv3 (Conv1D)                 (None, 15, 128)      98432       cConv2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "fConv4 (Conv1D)                 (None, 62, 128)      131200      fConv3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "cConv4 (Conv1D)                 (None, 15, 128)      98432       cConv3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "fMaxP2 (MaxPooling1D)           (None, 15, 128)      0           fConv4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "cMaxP2 (MaxPooling1D)           (None, 7, 128)       0           cConv4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "fFlat1 (Flatten)                (None, 1920)         0           fMaxP2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "cFlat1 (Flatten)                (None, 896)          0           cMaxP2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "merge_1 (Concatenate)           (None, 2816)         0           fFlat1[0][0]                     \n",
      "                                                                 cFlat1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mDrop1 (Dropout)                (None, 2816)         0           merge_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape1 (Reshape)              (None, 1, 2816)      0           mDrop1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_6 (LSTM)                   (None, 1, 64)        737536      reshape1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_7 (LSTM)                   (None, 64)           33024       lstm_6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "outLayer (Dense)                (None, 5)            325         lstm_7[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 1,336,613\n",
      "Trainable params: 1,336,613\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 765 samples, validate on 76 samples\n",
      "Epoch 1/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.5879 - acc: 0.2145\n",
      "Epoch 00001: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 8s 11ms/sample - loss: 1.5862 - acc: 0.2157 - val_loss: 1.6076 - val_acc: 0.2237\n",
      "Epoch 2/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.5433 - acc: 0.2259\n",
      "Epoch 00002: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.5429 - acc: 0.2248 - val_loss: 1.6053 - val_acc: 0.2237\n",
      "Epoch 3/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.5140 - acc: 0.2287\n",
      "Epoch 00003: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.5167 - acc: 0.2248 - val_loss: 1.6041 - val_acc: 0.3289\n",
      "Epoch 4/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4982 - acc: 0.2244\n",
      "Epoch 00004: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4961 - acc: 0.2248 - val_loss: 1.6035 - val_acc: 0.2632\n",
      "Epoch 5/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4815 - acc: 0.2202\n",
      "Epoch 00005: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4789 - acc: 0.2248 - val_loss: 1.6035 - val_acc: 0.2500\n",
      "Epoch 6/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4686 - acc: 0.2244\n",
      "Epoch 00006: val_loss did not improve from 1.60878\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4648 - acc: 0.2248 - val_loss: 1.6030 - val_acc: 0.2237\n",
      "Epoch 7/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4580 - acc: 0.2230\n",
      "Epoch 00007: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4552 - acc: 0.2248 - val_loss: 1.6030 - val_acc: 0.1974\n",
      "Epoch 8/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4528 - acc: 0.2230\n",
      "Epoch 00008: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4529 - acc: 0.2248 - val_loss: 1.6030 - val_acc: 0.1842\n",
      "Epoch 9/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4532 - acc: 0.2216\n",
      "Epoch 00009: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4512 - acc: 0.2248 - val_loss: 1.6030 - val_acc: 0.1842\n",
      "Epoch 10/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4500 - acc: 0.2202\n",
      "Epoch 00010: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4497 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 11/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4485 - acc: 0.2259\n",
      "Epoch 00011: val_loss did not improve from 1.60878\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4481 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4504 - acc: 0.2244\n",
      "Epoch 00012: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4493 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 13/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4467 - acc: 0.2273\n",
      "Epoch 00013: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4472 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 14/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4488 - acc: 0.2230\n",
      "Epoch 00014: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4472 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 15/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4524 - acc: 0.2230\n",
      "Epoch 00015: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 2s 2ms/sample - loss: 1.4467 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 16/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4469 - acc: 0.2244\n",
      "Epoch 00016: val_loss did not improve from 1.60878\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4469 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 17/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4423 - acc: 0.2315\n",
      "Epoch 00017: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4474 - acc: 0.2261 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 18/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4527 - acc: 0.2216\n",
      "Epoch 00018: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4472 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 19/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4450 - acc: 0.2273\n",
      "Epoch 00019: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4487 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 20/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4464 - acc: 0.2230\n",
      "Epoch 00020: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 2s 2ms/sample - loss: 1.4459 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 21/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4492 - acc: 0.2230\n",
      "Epoch 00021: val_loss did not improve from 1.60878\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1.0000000116860975e-08.\n",
      "765/765 [==============================] - 2s 2ms/sample - loss: 1.4472 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 22/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4514 - acc: 0.2188\n",
      "Epoch 00022: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 2s 2ms/sample - loss: 1.4474 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 23/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4488 - acc: 0.2273\n",
      "Epoch 00023: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4477 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 24/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4453 - acc: 0.2244\n",
      "Epoch 00024: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 2s 2ms/sample - loss: 1.4468 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 25/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4480 - acc: 0.2244\n",
      "Epoch 00025: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 2s 2ms/sample - loss: 1.4470 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 26/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4430 - acc: 0.2273\n",
      "Epoch 00026: val_loss did not improve from 1.60878\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 9.999999939225292e-10.\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4462 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 27/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4498 - acc: 0.2202\n",
      "Epoch 00027: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 2s 2ms/sample - loss: 1.4477 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 28/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4495 - acc: 0.2216\n",
      "Epoch 00028: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 2s 3ms/sample - loss: 1.4464 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 29/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4417 - acc: 0.2330\n",
      "Epoch 00029: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4483 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 30/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4513 - acc: 0.2188\n",
      "Epoch 00030: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4463 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 31/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4552 - acc: 0.2159\n",
      "Epoch 00031: val_loss did not improve from 1.60878\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 9.999999717180686e-11.\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4471 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 32/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4468 - acc: 0.2273\n",
      "Epoch 00032: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4471 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 33/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4526 - acc: 0.2188\n",
      "Epoch 00033: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4464 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 34/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4479 - acc: 0.2216\n",
      "Epoch 00034: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4484 - acc: 0.2261 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 35/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4454 - acc: 0.2287\n",
      "Epoch 00035: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 2s 2ms/sample - loss: 1.4470 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 36/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4450 - acc: 0.2287\n",
      "Epoch 00036: val_loss did not improve from 1.60878\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 9.99999943962493e-12.\n",
      "765/765 [==============================] - 2s 2ms/sample - loss: 1.4465 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 37/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4449 - acc: 0.2259\n",
      "Epoch 00037: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 2s 2ms/sample - loss: 1.4472 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 38/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4431 - acc: 0.2273\n",
      "Epoch 00038: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 2s 2ms/sample - loss: 1.4468 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 39/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4504 - acc: 0.2244\n",
      "Epoch 00039: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4487 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 40/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4447 - acc: 0.2287\n",
      "Epoch 00040: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4490 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 41/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4402 - acc: 0.2287\n",
      "Epoch 00041: val_loss did not improve from 1.60878\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 9.999999092680235e-13.\n",
      "765/765 [==============================] - 2s 2ms/sample - loss: 1.4467 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 42/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4454 - acc: 0.2273\n",
      "Epoch 00042: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4469 - acc: 0.2261 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 43/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4523 - acc: 0.2188\n",
      "Epoch 00043: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4470 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 44/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4460 - acc: 0.2273\n",
      "Epoch 00044: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4481 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 45/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4468 - acc: 0.2244\n",
      "Epoch 00045: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4478 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 46/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4506 - acc: 0.2244\n",
      "Epoch 00046: val_loss did not improve from 1.60878\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 9.9999988758398e-14.\n",
      "765/765 [==============================] - 2s 2ms/sample - loss: 1.4462 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 47/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4424 - acc: 0.2273\n",
      "Epoch 00047: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 2s 2ms/sample - loss: 1.4463 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 48/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4482 - acc: 0.2259\n",
      "Epoch 00048: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 2s 2ms/sample - loss: 1.4459 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 49/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4440 - acc: 0.2287\n",
      "Epoch 00049: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4469 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 50/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4579 - acc: 0.2145\n",
      "Epoch 00050: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4472 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 51/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4484 - acc: 0.2216\n",
      "Epoch 00051: val_loss did not improve from 1.60878\n",
      "\n",
      "Epoch 00051: ReduceLROnPlateau reducing learning rate to 9.999999146890344e-15.\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4473 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 52/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4533 - acc: 0.2188\n",
      "Epoch 00052: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4471 - acc: 0.2261 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 53/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4469 - acc: 0.2287\n",
      "Epoch 00053: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4481 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 54/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4415 - acc: 0.2287\n",
      "Epoch 00054: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4469 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 55/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4417 - acc: 0.2259\n",
      "Epoch 00055: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4467 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 56/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4398 - acc: 0.2315\n",
      "Epoch 00056: val_loss did not improve from 1.60878\n",
      "\n",
      "Epoch 00056: ReduceLROnPlateau reducing learning rate to 9.999998977483753e-16.\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4472 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 57/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4451 - acc: 0.2287\n",
      "Epoch 00057: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4479 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 58/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4458 - acc: 0.2273\n",
      "Epoch 00058: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4457 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 59/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4474 - acc: 0.2259\n",
      "Epoch 00059: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4476 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 60/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4471 - acc: 0.2230\n",
      "Epoch 00060: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4481 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 61/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4517 - acc: 0.2202\n",
      "Epoch 00061: val_loss did not improve from 1.60878\n",
      "\n",
      "Epoch 00061: ReduceLROnPlateau reducing learning rate to 9.999998977483754e-17.\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4475 - acc: 0.2261 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 62/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4483 - acc: 0.2244\n",
      "Epoch 00062: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4469 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 63/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4490 - acc: 0.2244\n",
      "Epoch 00063: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4483 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 64/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4547 - acc: 0.2202\n",
      "Epoch 00064: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 2s 2ms/sample - loss: 1.4459 - acc: 0.2261 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 65/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4506 - acc: 0.2244\n",
      "Epoch 00065: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4468 - acc: 0.2261 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 66/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4499 - acc: 0.2188\n",
      "Epoch 00066: val_loss did not improve from 1.60878\n",
      "\n",
      "Epoch 00066: ReduceLROnPlateau reducing learning rate to 9.999998845134856e-18.\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4473 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 67/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4432 - acc: 0.2273\n",
      "Epoch 00067: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4461 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 68/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4452 - acc: 0.2273\n",
      "Epoch 00068: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4462 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 69/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4518 - acc: 0.2216\n",
      "Epoch 00069: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4474 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4402 - acc: 0.2301\n",
      "Epoch 00070: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4477 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 71/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4424 - acc: 0.2259\n",
      "Epoch 00071: val_loss did not improve from 1.60878\n",
      "\n",
      "Epoch 00071: ReduceLROnPlateau reducing learning rate to 9.999999010570977e-19.\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4464 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 72/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4418 - acc: 0.2315\n",
      "Epoch 00072: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4462 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 73/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4463 - acc: 0.2244\n",
      "Epoch 00073: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4473 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 74/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4501 - acc: 0.2202\n",
      "Epoch 00074: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4462 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 75/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4509 - acc: 0.2230\n",
      "Epoch 00075: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4471 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 76/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4439 - acc: 0.2273\n",
      "Epoch 00076: val_loss did not improve from 1.60878\n",
      "\n",
      "Epoch 00076: ReduceLROnPlateau reducing learning rate to 9.999999424161285e-20.\n",
      "765/765 [==============================] - 2s 2ms/sample - loss: 1.4474 - acc: 0.2261 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 77/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4428 - acc: 0.2287\n",
      "Epoch 00077: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 2s 2ms/sample - loss: 1.4464 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 78/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4470 - acc: 0.2287\n",
      "Epoch 00078: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4490 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 79/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4387 - acc: 0.2301\n",
      "Epoch 00079: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4469 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 80/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4482 - acc: 0.2230\n",
      "Epoch 00080: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 2s 2ms/sample - loss: 1.4457 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 81/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4544 - acc: 0.2173\n",
      "Epoch 00081: val_loss did not improve from 1.60878\n",
      "\n",
      "Epoch 00081: ReduceLROnPlateau reducing learning rate to 9.999999682655225e-21.\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4472 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 82/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4504 - acc: 0.2244\n",
      "Epoch 00082: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4482 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 83/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4488 - acc: 0.2244\n",
      "Epoch 00083: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 2s 2ms/sample - loss: 1.4475 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 84/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4540 - acc: 0.2173\n",
      "Epoch 00084: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 2s 2ms/sample - loss: 1.4469 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 85/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4516 - acc: 0.2188\n",
      "Epoch 00085: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4470 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 86/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4471 - acc: 0.2230\n",
      "Epoch 00086: val_loss did not improve from 1.60878\n",
      "\n",
      "Epoch 00086: ReduceLROnPlateau reducing learning rate to 9.999999682655225e-22.\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4463 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 87/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4540 - acc: 0.2216\n",
      "Epoch 00087: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4473 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 88/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4489 - acc: 0.2216\n",
      "Epoch 00088: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4471 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 89/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4375 - acc: 0.2315\n",
      "Epoch 00089: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4461 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 90/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4443 - acc: 0.2301\n",
      "Epoch 00090: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4485 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 91/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4462 - acc: 0.2230\n",
      "Epoch 00091: val_loss did not improve from 1.60878\n",
      "\n",
      "Epoch 00091: ReduceLROnPlateau reducing learning rate to 9.999999682655225e-23.\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4449 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 92/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4473 - acc: 0.2230\n",
      "Epoch 00092: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4458 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 93/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4449 - acc: 0.2301\n",
      "Epoch 00093: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4468 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 94/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4535 - acc: 0.2202\n",
      "Epoch 00094: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4478 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 95/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4452 - acc: 0.2273\n",
      "Epoch 00095: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4480 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 96/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4402 - acc: 0.2287\n",
      "Epoch 00096: val_loss did not improve from 1.60878\n",
      "\n",
      "Epoch 00096: ReduceLROnPlateau reducing learning rate to 9.999999682655227e-24.\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4476 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 97/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4436 - acc: 0.2287\n",
      "Epoch 00097: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4463 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 98/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4493 - acc: 0.2244\n",
      "Epoch 00098: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4467 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4522 - acc: 0.2202\n",
      "Epoch 00099: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4481 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 100/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4477 - acc: 0.2273\n",
      "Epoch 00100: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4459 - acc: 0.2261 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 101/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4490 - acc: 0.2230\n",
      "Epoch 00101: val_loss did not improve from 1.60878\n",
      "\n",
      "Epoch 00101: ReduceLROnPlateau reducing learning rate to 9.999999998199588e-25.\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4468 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 102/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4446 - acc: 0.2287\n",
      "Epoch 00102: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4474 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 103/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4494 - acc: 0.2230\n",
      "Epoch 00103: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4479 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 104/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4565 - acc: 0.2173\n",
      "Epoch 00104: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 2s 2ms/sample - loss: 1.4465 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 105/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4440 - acc: 0.2259\n",
      "Epoch 00105: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 2s 2ms/sample - loss: 1.4468 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 106/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4499 - acc: 0.2244\n",
      "Epoch 00106: val_loss did not improve from 1.60878\n",
      "\n",
      "Epoch 00106: ReduceLROnPlateau reducing learning rate to 1.0000000195414814e-25.\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4489 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 107/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4510 - acc: 0.2188\n",
      "Epoch 00107: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4452 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 108/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4508 - acc: 0.2216\n",
      "Epoch 00108: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4471 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 109/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4469 - acc: 0.2259\n",
      "Epoch 00109: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4479 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 110/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4479 - acc: 0.2230\n",
      "Epoch 00110: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4479 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 111/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4494 - acc: 0.2216\n",
      "Epoch 00111: val_loss did not improve from 1.60878\n",
      "\n",
      "Epoch 00111: ReduceLROnPlateau reducing learning rate to 1.0000000195414814e-26.\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4457 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 112/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4449 - acc: 0.2273\n",
      "Epoch 00112: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4463 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 113/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4505 - acc: 0.2202\n",
      "Epoch 00113: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 2s 2ms/sample - loss: 1.4472 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 114/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4531 - acc: 0.2173\n",
      "Epoch 00114: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4481 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 115/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4567 - acc: 0.2131\n",
      "Epoch 00115: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4459 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 116/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4508 - acc: 0.2216\n",
      "Epoch 00116: val_loss did not improve from 1.60878\n",
      "\n",
      "Epoch 00116: ReduceLROnPlateau reducing learning rate to 9.999999887266024e-28.\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4463 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 117/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4450 - acc: 0.2259\n",
      "Epoch 00117: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4471 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 118/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4364 - acc: 0.2287\n",
      "Epoch 00118: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4470 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 119/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4493 - acc: 0.2273\n",
      "Epoch 00119: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4471 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n",
      "Epoch 120/120\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.4471 - acc: 0.2259\n",
      "Epoch 00120: val_loss did not improve from 1.60878\n",
      "765/765 [==============================] - 1s 2ms/sample - loss: 1.4476 - acc: 0.2248 - val_loss: 1.6029 - val_acc: 0.1842\n"
     ]
    }
   ],
   "source": [
    "model_2019 = model_baseline_2019()\n",
    "hist_19 = model_2019.fit(\n",
    "    pp_X_train, y_train_, batch_size=64, epochs=120, validation_data=(pp_X_val, y_val_), callbacks=callbacks_list\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> f1 score: 0.25336105336105336\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.81      0.70        16\n",
      "           1       0.00      0.00      0.00         5\n",
      "           2       0.40      0.96      0.56        23\n",
      "           3       0.00      0.00      0.00        20\n",
      "           4       0.00      0.00      0.00        12\n",
      "\n",
      "    accuracy                           0.46        76\n",
      "   macro avg       0.20      0.35      0.25        76\n",
      "weighted avg       0.25      0.46      0.32        76\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model_2019.predict(pp_X_test, batch_size=bs)\n",
    "y_pred = np.array([np.argmax(s) for s in y_pred])\n",
    "f1 = f1_score(y_test, y_pred, average=\"macro\")\n",
    "print(\">>> f1 score: {}\".format(f1))\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4605263157894737\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inLayer (InputLayer)            [(None, 3000, 1)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "fConv1 (Conv1D)                 (None, 500, 64)      3264        inLayer[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "cConv1 (Conv1D)                 (None, 60, 32)       12832       inLayer[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "fMaxP1 (MaxPooling1D)           (None, 62, 64)       0           fConv1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "cMaxP1 (MaxPooling1D)           (None, 15, 32)       0           cConv1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "fDrop1 (Dropout)                (None, 62, 64)       0           fMaxP1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "cDrop1 (Dropout)                (None, 15, 32)       0           cMaxP1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "fConv2 (Conv1D)                 (None, 62, 128)      65664       fDrop1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "cConv2 (Conv1D)                 (None, 15, 128)      24704       cDrop1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "fConv3 (Conv1D)                 (None, 62, 128)      131200      fConv2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "cConv3 (Conv1D)                 (None, 15, 128)      98432       cConv2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "fConv4 (Conv1D)                 (None, 62, 128)      131200      fConv3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "cConv4 (Conv1D)                 (None, 15, 128)      98432       cConv3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "fMaxP2 (MaxPooling1D)           (None, 15, 128)      0           fConv4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "cMaxP2 (MaxPooling1D)           (None, 7, 128)       0           cConv4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "fFlat1 (Flatten)                (None, 1920)         0           fMaxP2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "cFlat1 (Flatten)                (None, 896)          0           cMaxP2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "merge_1 (Concatenate)           (None, 2816)         0           fFlat1[0][0]                     \n",
      "                                                                 cFlat1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mDrop1 (Dropout)                (None, 2816)         0           merge_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape1 (Reshape)              (None, 1, 2816)      0           mDrop1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_10 (Bidirectional (None, 256)          3015680     reshape1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_5 (Reshape)             (None, 1, 256)       0           bidirectional_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_11 (Bidirectional (None, 256)          394240      reshape_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "merge_out_sub (Dropout)         (None, 256)          0           bidirectional_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "outLayer (Dense)                (None, 5)            1285        merge_out_sub[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 3,976,933\n",
      "Trainable params: 3,976,933\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 765 samples, validate on 76 samples\n",
      "Epoch 1/30\n",
      "704/765 [==========================>...] - ETA: 0s - loss: 1.5917 - acc: 0.2315\n",
      "Epoch 00001: val_loss improved from 1.60818 to 1.60878, saving model to model_cps\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Attempted to save a function b'__inference_forward_bLstm1_layer_call_fn_85211' which references a symbolic Tensor Tensor(\"dropout/mul_1:0\", shape=(None, 2816), dtype=float32) that is not a simple constant. This is not supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-180-af3f71dadbe7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel_2017\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_baseline_2017\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m hist2 = model_2017.fit(\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mpp_X_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpp_X_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m )\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    395\u001b[0m                       total_epochs=1)\n\u001b[0;32m    396\u001b[0m                   cbks.make_logs(model, epoch_logs, eval_result, ModeKeys.TEST,\n\u001b[1;32m--> 397\u001b[1;33m                                  prefix='val_')\n\u001b[0m\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m                 \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[1;34m(self, epoch, mode)\u001b[0m\n\u001b[0;32m    769\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    770\u001b[0m         \u001b[1;31m# Epochs only apply to `fit`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 771\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    772\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    773\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    300\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m       \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    990\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    991\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 992\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    993\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    994\u001b[0m       \u001b[1;31m# For multi-worker training, back up the weights and current training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_save_model\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   1027\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1028\u001b[0m               \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1029\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1030\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\network.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, filepath, overwrite, include_optimizer, save_format, signatures, options)\u001b[0m\n\u001b[0;32m   1006\u001b[0m     \"\"\"\n\u001b[0;32m   1007\u001b[0m     save.save_model(self, filepath, overwrite, include_optimizer, save_format,\n\u001b[1;32m-> 1008\u001b[1;33m                     signatures, options)\n\u001b[0m\u001b[0;32m   1009\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1010\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0msave_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\save.py\u001b[0m in \u001b[0;36msave_model\u001b[1;34m(model, filepath, overwrite, include_optimizer, save_format, signatures, options)\u001b[0m\n\u001b[0;32m    113\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m     saved_model_save.save(model, filepath, overwrite, include_optimizer,\n\u001b[1;32m--> 115\u001b[1;33m                           signatures, options)\n\u001b[0m\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\saved_model\\save.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(model, filepath, overwrite, include_optimizer, signatures, options)\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[1;31m# we use the default replica context here.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mdistribution_strategy_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_default_replica_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m       \u001b[0msave_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msignatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0minclude_optimizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\saved_model\\save.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(obj, export_dir, signatures, options)\u001b[0m\n\u001b[0;32m    907\u001b[0m   \u001b[0mobject_saver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrackableSaver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheckpoint_graph_view\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    908\u001b[0m   asset_info, exported_graph = _fill_meta_graph_def(\n\u001b[1;32m--> 909\u001b[1;33m       meta_graph_def, saveable_view, signatures, options.namespace_whitelist)\n\u001b[0m\u001b[0;32m    910\u001b[0m   saved_model.saved_model_schema_version = (\n\u001b[0;32m    911\u001b[0m       constants.SAVED_MODEL_SCHEMA_VERSION)\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\saved_model\\save.py\u001b[0m in \u001b[0;36m_fill_meta_graph_def\u001b[1;34m(meta_graph_def, saveable_view, signature_functions, namespace_whitelist)\u001b[0m\n\u001b[0;32m    551\u001b[0m   \u001b[0mresource_initializer_ops\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m   \u001b[1;32mwith\u001b[0m \u001b[0mexported_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 553\u001b[1;33m     \u001b[0mobject_map\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresource_map\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0masset_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msaveable_view\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_resources\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    554\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mresource_initializer_function\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mresource_initializer_functions\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    555\u001b[0m       \u001b[0masset_dependencies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\saved_model\\save.py\u001b[0m in \u001b[0;36mmap_resources\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    283\u001b[0m                 (\"Attempted to save a function {} which references a symbolic \"\n\u001b[0;32m    284\u001b[0m                  \u001b[1;34m\"Tensor {} that is not a simple constant. This is not \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 285\u001b[1;33m                  \"supported.\").format(concrete_function.name, capture))\n\u001b[0m\u001b[0;32m    286\u001b[0m           \u001b[0mcopied_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcapture_constant_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m           \u001b[0mnode_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Attempted to save a function b'__inference_forward_bLstm1_layer_call_fn_85211' which references a symbolic Tensor Tensor(\"dropout/mul_1:0\", shape=(None, 2816), dtype=float32) that is not a simple constant. This is not supported."
     ]
    }
   ],
   "source": [
    "model_2017 = model_baseline_2017()\n",
    "hist2 = model_2017.fit(\n",
    "    pp_X_train, y_train_, batch_size=64, epochs=30, validation_data=(pp_X_val, y_val_), callbacks=callbacks_list\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> f1 score: 0.06956521739130435\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.21      1.00      0.35        16\n",
      "           1       0.00      0.00      0.00         5\n",
      "           2       0.00      0.00      0.00        23\n",
      "           3       0.00      0.00      0.00        20\n",
      "           4       0.00      0.00      0.00        12\n",
      "\n",
      "    accuracy                           0.21        76\n",
      "   macro avg       0.04      0.20      0.07        76\n",
      "weighted avg       0.04      0.21      0.07        76\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model_2017.predict(pp_X_test, batch_size=bs)\n",
    "y_pred = np.array([np.argmax(s) for s in y_pred])\n",
    "f1 = f1_score(y_test, y_pred, average=\"macro\")\n",
    "print(\">>> f1 score: {}\".format(f1))\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21052631578947367"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2948067aec8>]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hist2.history[\"loss\"])\n",
    "plt.plot(hist2.history[\"val_loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x294806e6288>]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hist2.history[\"acc\"])\n",
    "plt.plot(hist2.history[\"val_acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bs = 128\n",
    "# epochs = 5\n",
    "# hists = []\n",
    "\n",
    "# for i in range(epochs):\n",
    "#     print(\">>> Epochs {}\".format(i + 1))\n",
    "#     hist = model_2017.fit_generator(\n",
    "#         data_generator(X_train, y_train_, bs=bs),\n",
    "#         validation_data=data_generator(X_val, y_val_, bs=bs),\n",
    "#         epochs=1,\n",
    "#         steps_per_epoch=len(X_train) // bs,\n",
    "#         validation_steps=len(X_val) // bs,\n",
    "#         callbacks=callbacks_list\n",
    "#     )\n",
    "#     y_pred = model_2017.predict(X_test, batch_size=bs)\n",
    "#     y_pred = np.array([np.argmax(s) for s in y_pred])\n",
    "# #     f1 = f1_score(y_test, y_pred, average=\"macro\")\n",
    "# #     print(\">>> f1 score: {}\".format(f1))\n",
    "#     report = classification_report(y_test, y_pred)\n",
    "#     print(report)\n",
    "#     hists.append(hist.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2017.save_weights(\"pp_model_2017.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model_2017.to_json()\n",
    "with open(\"eeg_model_2017_config.json\", \"w\") as f:\n",
    "    f.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'ls' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!ls log_training.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_name</th>\n",
       "      <th>config</th>\n",
       "      <th>keras_version</th>\n",
       "      <th>backend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>input_layers</th>\n",
       "      <td>Model</td>\n",
       "      <td>[[inLayer, 0, 0]]</td>\n",
       "      <td>2.2.4-tf</td>\n",
       "      <td>tensorflow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layers</th>\n",
       "      <td>Model</td>\n",
       "      <td>[{'class_name': 'InputLayer', 'config': {'batc...</td>\n",
       "      <td>2.2.4-tf</td>\n",
       "      <td>tensorflow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <td>Model</td>\n",
       "      <td>model_2</td>\n",
       "      <td>2.2.4-tf</td>\n",
       "      <td>tensorflow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>output_layers</th>\n",
       "      <td>Model</td>\n",
       "      <td>[[outLayer, 0, 0]]</td>\n",
       "      <td>2.2.4-tf</td>\n",
       "      <td>tensorflow</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              class_name                                             config  \\\n",
       "input_layers       Model                                  [[inLayer, 0, 0]]   \n",
       "layers             Model  [{'class_name': 'InputLayer', 'config': {'batc...   \n",
       "name               Model                                            model_2   \n",
       "output_layers      Model                                 [[outLayer, 0, 0]]   \n",
       "\n",
       "              keras_version     backend  \n",
       "input_layers       2.2.4-tf  tensorflow  \n",
       "layers             2.2.4-tf  tensorflow  \n",
       "name               2.2.4-tf  tensorflow  \n",
       "output_layers      2.2.4-tf  tensorflow  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "file = pd.read_json('eeg_model_2017_config.json')\n",
    "file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size=0.09,random_state= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=1, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(765, 3000)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With KNN (K=3) accuracy is:  0.15789473684210525\n"
     ]
    }
   ],
   "source": [
    "prediction = knn.predict(X_test)\n",
    "print('With KNN (K=3) accuracy is: ',knn.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_X_train = pp_X_train.reshape(680,3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=1, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.fit(pp_X_train, y_train_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_X_test = pp_X_test.reshape(85,3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85, 3000)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp_X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With KNN (K=3) accuracy is:  0.07058823529411765\n"
     ]
    }
   ],
   "source": [
    "prediction = knn.predict(pp_X_test)\n",
    "print('With KNN (K=3) accuracy is: ',knn.score(pp_X_test,y_test_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With NB accuracy is:  0.5394736842105263\n"
     ]
    }
   ],
   "source": [
    "nb = GaussianNB()\n",
    "nb.fit(X_train,y_train)\n",
    "prediction = nb.predict(X_test)\n",
    "\n",
    "print('With NB accuracy is: ',nb.score(X_test,y_test)) # accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With decision tree accuracy is:  0.27631578947368424\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dtree = DecisionTreeClassifier()\n",
    "dtree.fit(X_train,y_train)\n",
    "prediction = dtree.predict(X_test)\n",
    "\n",
    "print('With decision tree accuracy is: ',dtree.score(X_test,y_test)) # accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With Random forest accuracy is:  0.5789473684210527\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train,y_train)\n",
    "prediction = rf.predict(X_test)\n",
    "\n",
    "print('With Random forest accuracy is: ',rf.score(X_test,y_test)) # accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With SVM accuracy is:  0.2236842105263158\n"
     ]
    }
   ],
   "source": [
    "\n",
    "svc = SVC(kernel='linear', C=1,gamma='auto')\n",
    "svc.fit(X, Y)\n",
    "svc.fit(X_train,y_train)\n",
    " \n",
    "prediction = svc.predict(X_test)\n",
    "\n",
    "print('With SVM accuracy is: ',svc.score(X_test,y_test)) # accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
